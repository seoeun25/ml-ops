{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Distributed training using pytorch operation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import NamedTuple\n",
    "from collections import namedtuple\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.dsl.types import Integer\n",
    "\n",
    "kfp.__version__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "ROOT_DIR = os.path.abspath('/home/jovyan')\n",
    "efs_mount_point = 'efs-data'\n",
    "train_image = '974643886555.dkr.ecr.ap-northeast-2.amazonaws.com/aladin-runtime:pytorch-llama'\n",
    "#train_image='public.ecr.aws/pytorch-samples/pytorch_dist_mnist:latest'\n",
    "train_script = 'klue_ynat_2_finetune_4_13b-chat.py'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copy the training script to EFS mount path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "efs_mount_dir=ROOT_DIR+'/' + efs_mount_point\n",
    "print(efs_mount_dir)\n",
    "!cp ../klue/$train_script $efs_mount_dir/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get kuberenetes pvc claim id for the provisioned efs from Kubeflow Volumes on the dashboard. eg. efs-sc-claim\n",
    "pvc_claim_id=!(kubectl get pvc --no-headers=true | awk '/efs-data/{print$3}' )\n",
    "pvc_claim_id[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GET kfp client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kfp_client():\n",
    "    \"\"\"\n",
    "    Returns Kubeflow pipelines client inside cluster.\n",
    "    \"\"\"\n",
    "    end_point=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"\n",
    "    credentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n",
    "    client = kfp.Client(host=end_point, credentials=credentials)\n",
    "\n",
    "    return client\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def base_image() -> str:\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    iam = os.environ.get(\"AWS_ROLE_ARN\")\n",
    "    account = re.findall(\"arn:aws:iam::(.*):role*\", iam)[0]\n",
    "    region = os.environ.get(\"AWS_REGION\")\n",
    "    base_image = \"{}.dkr.ecr.{}.amazonaws.com/aladin-runtime:anaconda-cpu\".format(account, region)\n",
    "    print(\"base_image = {}\".format(base_image))\n",
    "    return base_image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_current_namespace():\n",
    "    \"\"\"Returns current namespace if available, else kubeflow\"\"\"\n",
    "    try:\n",
    "        current_namespace = open(\n",
    "            \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "        ).read()\n",
    "    except:\n",
    "        current_namespace = \"kubeflow\"\n",
    "    return current_namespace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Worker spec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create worker spec\n",
    "def create_worker_spec(\n",
    "        worker_num: int = 0,\n",
    "        train_image_name: str = None,\n",
    "        train_script_name: str = None\n",
    ") -> NamedTuple(\n",
    "    \"CreatWorkerSpec\", [(\"worker_spec\", dict)]\n",
    "):\n",
    "    from collections import namedtuple\n",
    "    \"\"\"\n",
    "    Creates pytorch-job worker spec\n",
    "    \"\"\"\n",
    "    worker = {}\n",
    "    if worker_num > 0:\n",
    "        worker = {\n",
    "            \"replicas\": worker_num,\n",
    "            \"restartPolicy\": \"OnFailure\",\n",
    "            \"template\": {\n",
    "                \"metadata\": {\n",
    "                    \"annotations\": {\n",
    "                        \"sidecar.istio.io/inject\": \"false\"\n",
    "                    }\n",
    "                },\n",
    "                \"spec\": {\n",
    "                    \"containers\": [\n",
    "                        {\n",
    "                            \"command\": [\n",
    "                                \"sh\",\n",
    "                                \"-ec\",\n",
    "                                \"python3 -m pip install --user --no-warn-script-location tensorboardX && $0 $@\",\n",
    "                                \"python\",\n",
    "                                f\"/efs-data/{train_script_name}\"\n",
    "                            ],\n",
    "                            \"args\": [\n",
    "                                \"--backend\",\n",
    "                                \"gloo\",\n",
    "                            ],\n",
    "                            \"image\": f\"{train_image_name}\",\n",
    "                            \"name\": \"pytorch\",\n",
    "                            \"resources\": {\n",
    "                                \"requests\": {\n",
    "                                    \"memory\": \"40Gi\",\n",
    "                                    \"cpu\": \"7\",\n",
    "                                    # Uncomment for GPU\n",
    "                                    \"nvidia.com/gpu\": 2,\n",
    "                                }\n",
    "                            },\n",
    "                            \"volumeMounts\": [\n",
    "                                {\n",
    "                                    \"mountPath\": \"/efs-data\",\n",
    "                                    \"name\": \"efs-data\"\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                    \"volumes\": [\n",
    "                        {\n",
    "                            \"name\": \"efs-data\",\n",
    "                            \"persistentVolumeClaim\": {\n",
    "                                \"claimName\": \"efs-data\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "    worker_spec_output = namedtuple(\n",
    "        \"MyWorkerOutput\", [\"worker_spec\"]\n",
    "    )\n",
    "    return worker_spec_output(worker)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create worker_spec component\n",
    "worker_spec_op = components.func_to_container_op(\n",
    "    create_worker_spec,\n",
    "    base_image=base_image(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Kubeflow Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"klue-pytorchjob\",\n",
    "    description=\"An example to launch pytorch.\",\n",
    ")\n",
    "def train_def(\n",
    "        namespace: str = get_current_namespace(),\n",
    "        worker_replicas: int = 1,\n",
    "        ttl_seconds_after_finished: int = -1,\n",
    "        job_timeout_minutes: int = 60,\n",
    "        delete_after_done: bool = False,\n",
    "):\n",
    "    print(\"train_pipeline: namespace={}, worker_replicas={}, ttl_seconds_after_finished={}, job_timeout_minutes={}, delete_after_done={}\"\n",
    "          .format(namespace, worker_replicas, ttl_seconds_after_finished, job_timeout_minutes, delete_after_done))\n",
    "    pytorchjob_launcher_op = components.load_component_from_file(\n",
    "        \"../launcher/component.yaml\"\n",
    "    )\n",
    "\n",
    "    master = {\n",
    "        \"replicas\": 1,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"annotations\": {\n",
    "                    # See https://github.com/kubeflow/website/issues/2011\n",
    "                    \"sidecar.istio.io/inject\": \"false\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        #To override default command\n",
    "                        \"command\": [\n",
    "                            \"sh\",\n",
    "                            \"-ec\",\n",
    "                            \"| \\npython3 -m pip install --user --no-warn-script-location tensorboardX \\n$0 $@\",\n",
    "                            \"python\",\n",
    "                            f\"/efs-data/{train_script}\"\n",
    "                        ],\n",
    "                        \"args\": [\n",
    "                            \"--backend\",\n",
    "                            \"gloo\",\n",
    "                        ],\n",
    "                        # Or, create your own image from\n",
    "                        # https://github.com/kubeflow/pytorch-operator/tree/master/examples/mnist\n",
    "                        \"image\": f\"{train_image}\",\n",
    "                        \"name\": \"pytorch\",\n",
    "                        \"resources\": {\n",
    "                            \"requests\": {\n",
    "                                \"memory\": \"40Gi\",\n",
    "                                \"cpu\": \"7\",\n",
    "                                # Uncomment for GPU\n",
    "                                \"nvidia.com/gpu\": 2,\n",
    "                            }\n",
    "                        },\n",
    "                        \"volumeMounts\": [\n",
    "                            {\n",
    "                                \"mountPath\": \"/efs-data\",\n",
    "                                \"name\": \"efs-data\"\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                \"volumes\": [\n",
    "                    {\n",
    "                        \"name\": \"efs-data\",\n",
    "                        \"persistentVolumeClaim\": {\n",
    "                            \"claimName\": \"efs-data\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"master_spec:\\n{master}\")\n",
    "\n",
    "    worker_spec_create = worker_spec_op(\n",
    "        worker_replicas, train_image, train_script\n",
    "    )\n",
    "\n",
    "    # Launch and monitor the job with the launcher\n",
    "    pytorchjob_launcher_op(\n",
    "        # Note: name needs to be a unique pytorchjob name in the namespace.\n",
    "        # Using RUN_ID_PLACEHOLDER is one way of getting something unique.\n",
    "        name=f\"pytorch-klue-{kfp.dsl.RUN_ID_PLACEHOLDER}\",\n",
    "        namespace=namespace,\n",
    "        master_spec=master,\n",
    "        # pass worker_spec as a string because the JSON serializer will convert\n",
    "        # the placeholder for worker_replicas (which it sees as a string) into\n",
    "        # a quoted variable (eg a string) instead of an unquoted variable\n",
    "        # (number).  If worker_replicas is quoted in the spec, it will break in\n",
    "        # k8s.  See https://github.com/kubeflow/pipelines/issues/4776\n",
    "        worker_spec=worker_spec_create.outputs[\n",
    "            \"worker_spec\"\n",
    "        ],\n",
    "        ttl_seconds_after_finished=ttl_seconds_after_finished,\n",
    "        job_timeout_minutes=job_timeout_minutes,\n",
    "        delete_after_done=delete_after_done,\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload the pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "\n",
    "pipeline_file = \"pytorch_klue_pipeline.yaml\"\n",
    "print(\n",
    "    f\"Compiling pipeline as {pipeline_file}\"\n",
    ")\n",
    "compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V1_LEGACY).compile(\n",
    "    train_def, pipeline_file\n",
    ")\n",
    "client = kfp_client()\n",
    "client.upload_pipeline(pipeline_package_path=pipeline_file, pipeline_name=\"pytorch_klue_notebook\", description=\"pytorch_klue_notebook\")\n",
    "print(f\"Created pipeline \")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
